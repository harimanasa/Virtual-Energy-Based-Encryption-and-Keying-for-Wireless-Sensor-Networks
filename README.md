1 INTRODUCTION:
Rapidly developed WSN technology is no longer nascent and will be used in a variety of application scenarios. Typical application areas include environmental, military, and commercial enterprises. For example, in a battlefield scenario, sensors may be used to detect the location of enemy sniper fire or to detect harmful chemical agents before they reach troops. In another potential scenario, sensor nodes forming a network under water could be used for oceanographic data collection, pollution monitoring, assisted navigation, military surveillance, and mine reconnaissance operations. Future improvements in technology will bring more sensor applications into our daily lives and the use of sensors will also evolve from merely capturing data to a system that can be used for real-time compound event alerting.

From a security standpoint, it is very important to provide authentic and accurate data to surrounding sensor nodes and to the sink to trigger time-critical responses (e.g., troop movement, evacuation, and first response deployment). Protocols should be resilient against false data injected into the network by malicious nodes. Otherwise, consequences for propagating false data or redundant data are costly, depleting limited network resources and wasting response efforts. However, securing sensor networks poses unique challenges to protocol builders because these tiny wireless devices are deployed in large numbers, usually in unattended environments, and are severely limited in their capabilities and resources (e.g., power, computational capacity, and memory). For instance, a typical sensor operates at the frequency of 2.4 GHz, has a data rate of 250 Kbps, 128 KB of program flash memory, 512 KB of memory for measurements, transmit power between 100 _W and 1 mW, and a communications range of 30 to 100 m. Therefore, protocol builders must be cautious about utilizing the limited resources onboard the sensors efficiently.

In this paper, we focus on keying mechanisms for WSNs. There are two fundamental key management schemes for WSNs: static and dynamic. In static key management schemes, key management functions (i.e., key generation and distribution) are handled statically. That is, the sensors have a fixed number of keys loaded either prior to or shortly after network deployment. On the other hand, dynamic key management schemes perform keying functions (rekeying) either periodically or on demand as needed by the network. The sensors dynamically exchange keys to communicate. Although dynamic schemes are more attack resilient than static ones, one significant disadvantage is that they increase the communication overhead due to keys being refreshed or redistributed from time to time in the network. There are many reasons for key refreshment, including: updating keys after a key revocation has occurred, refreshing the key such that it does not become stale, or changing keys due to dynamic changes in the topology. 

In this paper, we seek to minimize the overhead associated with refreshing keys to avoid them becoming stale. Because the communication cost is the most dominant factor in a sensor’s energy consumption the message transmission cost for rekeying is an important issue in a WSN deployment (as analyzed in the next section). Furthermore, for certain WSN applications (e.g., military applications), it may be very important to minimize the number of messages to decrease the probability of detection if deployed in an enemy territory. That is, being less “chatty” intuitively decreases the number of opportunities for malicious entities to eavesdrop or intercept packets. The purpose of this paper is to develop an efficient and secure communication framework for WSN applications. Specifically, in this paper, we introduce Virtual Energy- Based Encryption and Keying (VEBEK) for WSNs, which is primarily inspired by our previous work. VEBEK’s secure communication framework provides a technique to verify data in line and drop false packets from malicious nodes, thus maintaining the health of the sensor network. VEBEK dynamically updates keys without exchanging messages for key renewals and embeds integrity into packets as opposed to enlarging the packet by appending message authentication codes (MACs). Specifically, each sensed data is protected using a simple encoding scheme based on a permutation code generated with the RC4 encryption scheme and sent toward the sink. The key to the encryption scheme dynamically changes as a function of the residual virtual energy of the sensor, thus requiring no need for rekeying. 

Therefore, a one-time dynamic key is used for one message generated by the source sensor and different keys are used for the successive packets of the stream. The nodes forwarding the data along the path to the sink are able to verify the authenticity and integrity of the data and to provide non repudiation. The protocol is able to continue its operations under dire communication cases as it may be operating in a high-error-prone deployment area like under water. VEBEK unbundles key generation from other security services, namely authentication, integrity, and non repudiation; thus, its flexible modular architecture allows for adoption of other encryption mechanisms. The paper proceeds as follows: To motivate our work, a preliminary analysis of the rekeying cost with and without explicit control messages is given in discusses the semantics of VEBEK. VEBEK’s different operational modes are discussed in analytical framework and performance evaluation results including a comparison with other relevant works summarizes the design rationale and benefits of the VEBEK framework.
 
2 SYSTEM ANALYSIS
2.1 EXISTING SYSTEM:

An existing Dynamic Energy-based Encoding and Filtering framework to detect the injection of false data into a sensor network. Dynamic Energy-based that each sensed event report be encoded using a simple encoding scheme based on a keyed hash. The key to the hashing function dynamically changes as a function of the transient energy of the sensor, thus requiring no need for re-keying. Depending on the cost of transmission vs. computational cost of encoding, it may be important to remove data as quickly as possible. Accordingly, DEEF can provide authentication at the edge of the network or authentication inside of the sensor network. Depending on the optimal configuration, as the report is forwarded, each node along the way verifies the correctness of the encoding probabilistically and drops those that are invalid. We have evaluated DEEF's feasibility and performance through analysis our results show that DEEF, without incurring transmission overhead.   
 

2.2 PROPOSED SYSTEM:

VEBEK is a secure communication framework where sensed data is encoded using a scheme based on a permutation code generated via the RC4 encryption mechanism. The key to the RC4 encryption mechanism dynamically changes as a function of the residual virtual energy of the sensor. Thus, a one-time dynamic key is employed for one packet only and different keys are used for the successive packets of the stream. The intermediate nodes along the path to the sink are able to verify the authenticity and integrity of the incoming packets using a predicted value of the key generated by the sender’s virtual energy, thus requiring no need for specific rekeying messages. Our results show that VEBEK, without incurring transmission overhead (increasing packet size or sending control messages for rekeying), is able to eliminate malicious data from the network in an energy efficient manner.  The encoding operation is essentially the process of permutation of the bits in the packet, according to the dynamically created permutation code via the RC4 encryption mechanism. The key to RC4 is created by the previous module (virtual energy-based keying module). The purpose of the crypto module is to provide simple confidentiality of the packet header and payload while ensuring the authenticity and integrity of sensed data without incurring transmission overhead of traditional schemes. However, since the key generation and handling process is done in another module, VEBEK’s flexible architecture allows for adoption of stronger encryption mechanisms in lieu of encoding. We also show that our framework performs better than other comparable schemes in the literature with an overall 60-100 percent improvement in energy savings without the assumption of a reliable medium access control layer.





2.3 HARDWARE REQUIREMENTS:
System			: 	Pentium IV 2.4 GHz.
Hard Disk		: 	40 GB.
Floppy Drive		: 	1.44 Mb.
Monitor		: 	15 VGA Color.
Mouse			: 	Logitech.
Ram			: 	512 Mb.

2.4 SOFTWARE REQUIREMENTS:
Operating System 	:	Windows XP Professional
Coding Language	:  	Visual C# .Net.
 

2.5 MODULE DESCRIPTION:
VIRTUAL ENERGY-BASED KEYING:
•	The virtual energy-based keying module of the VEBEK framework is one of the primary contributions of this paper. It is essentially the method used for handling the keying process. It produces a dynamic key that is then fed into the crypto module. 
•	In VEBEK, each sensor node has a certain virtual energy value when it is first deployed in the network. Sensor nodes traverse several functional states. The states mainly include node-stay-alive, packet reception, transmission, encoding, and decoding. As each of these actions occurs, the virtual energy in a sensor node is depleted. The current value of the virtual energy in the node is used as the key to the key generation function. 
•	VEBEK’s virtual energy-based keying module ensures that each detected packet2 is associated with a new unique key generated based on the transient value of the virtual energy. After the dynamic key is generated, it is passed to the crypto module, where the desired security services are implemented.
•	Since a sensor node will be either forwarding some other sensor’s data or injecting its own data into the network, the set of actions and their associated energies for VEBEK includes packet reception (Erx), packet transmission (Etx), packet encoding (Eenc), packet decoding (Edec) energies, and the energy required to keep a node alive in the idle state (Ea). Specifically, the transient value of the virtual energy, Ev, is computed by decrementing the total of these predefined associated costs, Evc, from the previous virtual energy value.
In VEBEK, each sensor node has a certain virtual energy value when it is first deployed in the network. After deployment, sensor nodes traverse several functional states. The states mainly include node-stay-alive, packet reception, transmission, encoding, and decoding. As each of these actions occur, the virtual energy in a sensor node is depleted. The current value of the virtual energy, Evc, in the node is used as the key to the key generation function, F. During the initial deployment, each sensor node will have the same energy level Eini, therefore, the initial key, K1, is a function of the initial virtual energy value and an initialization vector (IV ). The IV s are predistributed to the sensors. Subsequent keys, Kj, are a function of the current virtual energy, Evc, and the previous key Kj_1. VEBEK’s virtual energy-based keying module ensures that each detected packet2 is associated with a new unique key generated based on the transient value of the virtual energy. After the dynamic key is generated, it is passed to the crypto module, where the desired security services are implemented. 
 
TABLE 1 Notations Used

The process of key generation is initiated when data is sensed; thus, no explicit mechanism is needed to refresh or update keys. Moreover, the dynamic nature of the keys makes it difficult for attackers to intercept enough packets to break the encoding algorithm. The details are given in Algorithm 1. As mentioned above, each node computes and updates the transient value of its virtual energy after performing some actions. Each action (or state traversal) on a node is associated with a certain predetermined cost. Since a sensor node will be either forwarding some other sensor’s data or injecting its own data into the network, the set of actions and their associated energies for VEBEK includes packet reception (Erx), packet transmission (Etx), packet encoding (Eenc), packet decoding (Edec) energies, and the energy required to keep a node alive in the idle state (Ea).Specifically, the transient value of the virtual energy, Ev, is computed by decrementing the total of these predefined associated costs, Evc, from the previous virtual energy value.
Algorithm 1. Compute Dynamic Key
1: Compute Dynamic Key (Evc, IDclr)
2: begin
3: j <- txIDpower(clr)base(cnt)
4: if j =1 then
5: Kj <-F(Eini,IV)
6: else
7: Kj <- F(K(j_1), Evc)
8: end if
9: return Kj
10end
The exact procedure to compute virtual cost, Evc, slightly differs if a sensor node is the originator of the data or the forwarder (i.e., receiver of data from another sensor). In
order to successfully decode and authenticate a packet, a receiving node must keep track of the energy of the sending node to derive the key needed for decoding. In VEBEK, the operation of tracking the energy of the sending node at the receiver is called watching and the energy value that is associated with the watched sensor is called Virtual Perceived Energy (Ep)
More formal definitions for watching are given as follows:
Definition 1
Given a finite number of sensor nodes, N(N={1, . . .,N}), deployed in a region, watching is defined as a node’s responsibility for monitoring and filtering packets coming from a certain (configurable) number of sensor nodes, r, where r <=N. <. is used to denote the watching operation.
Definition 2
Given a sensor node i, the total number of watched nodes, r, which the node is configured to watch, constitutes a watching list, WLi for node i and WLi =(1, 2, . . . ; r). Node i watches node k if IDk belongs WLi.
Deciding which nodes to watch and how many depends on the preferred configuration of the VEBEK authentication algorithm, which we designate as the operational mode of the framework. Specifically, we propose two operational
modes VEBEK-I and VEBEK-II.
When an event is detected by a source sensor, that node has remained alive for t units of time since the last event (or since the network deployment if this is the first event detected). After detection of the event, the node sends the l-bit length packet toward the sink. In this case, the following is the virtual cost associated with the source node:
Evc = l * (etx + eenc) + t * ea + Esynch:
In the case where a node receives data from another node, the virtual perceived energy value can be updated by decrementing the cost associated with the actions performed
by the sending node using the following cost equation. Thus, assuming that the receiving node has the initial virtual energy value of the sending node and that the packet is successfully received and decoded associated with a given source sensor, k, the virtual cost of the perceived energy is computed as follows:
Ekp= l * (erx + edec + etx + eenc) + t * 2 * ea;
where in both the equations, the small es refer to the one bit energy costs of the associated parameter.However, Esynch refers to a value to synchronize the source with the watcher-forwarders toward the sink as watcher-forwarder nodes spend more virtual energy due to packet reception and decoding operations, which are not present in source nodes.
Hence, Esynch = l * (erx + edec) + ea * t. The watching concept is illustrated with an example in Fig. 1. 
 
Fig.1 An illustration of the watching concept with forwarding.
In the above figure, there is one source sensor node, A, and other nodes B, C, and D are located along the path to the sink. Every node watches its downstream node, i.e., B watches A (B <_ A), C watches B (C <_ B), and D watches C (D <_ C). All the nodes have the initial virtual energy of 2,000 mJ and as packets are inserted into the network from the source node (A) overtime, nodes decrement their virtual energy values. For instance, as shown in Fig. 1, node A starts with the value of 2,000 mJ as the first key to encode the packet (key generation based on the virtual energies is explained in the crypto module). Node A sends the first packet and decrements its virtual energy to 1,998 mJ. After node B receives this first packet, it uses the virtual perceived energy value (Ep = 2;000 mJ) as the key to decode the packet, and updates its Ep (1,998 mJ) after sending the packet. When the packet travels up to the sink, the virtual energy becomes a shared dynamic cryptic credential among the nodes.

RESOURCE CRYPTO MODULE:

•	The crypto module in VEBEK employs a simple encoding process, which is essentially the process of permutation of the bits in the packet according to the dynamically created permutation code generated via RC4. The encoding is a simple encryption mechanism adopted for VEBEK. 

•	However, VEBEK’s flexible architecture allows for adoption of stronger encryption mechanisms in lieu of encoding. Last, the forwarding module handles the process of sending or receiving of encoded packets along the path to the sink. After the dynamic key is generated, it is passed to the crypto module, where the desired security services are implemented. 

•	The process of key generation is initiated when data is sensed; thus, no explicit mechanism is needed to refresh or update keys. Moreover, the dynamic nature of the keys makes it difficult for attackers to intercept enough packets to break the encoding algorithm. 

•	Due to the resource constraints of WSNs, traditional digital signatures or encryption mechanisms requiring expensive cryptography is not viable. The scheme must be simple, yet effective. Thus, in this section, we introduce a simple encoding operation. The encoding operation is essentially the process of permutation of the bits in the packet, according to the dynamically created permutation code via the RC4 encryption mechanism.
Due to the resource constraints of WSNs, traditional digital signatures or encryption mechanisms requiring expensive cryptography is not viable. The scheme must be simple, yet effective. Thus, in this section, we introduce a simple encoding operation. The encoding operation is essentially the process of permutation of the bits in the packet, according to the dynamically created permutation code via the RC4 encryption mechanism. The key to RC4 is created by the previous module (virtual energy-based keying module). The purpose of the crypto module is to provide simple confidentiality of the packet header and payload while ensuring the authenticity and integrity of sensed data without incurring transmission overhead of traditional schemes. However, since the key
generation and handling process is done in another module, VEBEK’s flexible architecture allows for adoption of stronger encryption mechanisms in lieu of encoding.
The packets in VEBEK consists of the ID (i-bits), type (t-bits) (assuming each node has a type identifier), and data (d-bits) fields. Each node sends these to its next hop.
However, the sensors’ ID, type, and the sensed data are transmitted in a pseudorandom fashion according to the result of RC4. More specifically, the RC4 encryption algorithm takes the key and the packet fields (byte-bybyte) as inputs and produces the result as a permutation code as depicted in Fig. 2. The concatenation of each 8-bit output becomes the resultant permutation code. As mentioned earlier, the key to the RC4 mechanism is taken rom the core virtual energy-based keying module, which is responsible for generating the dynamic key according to the residual virtual energy level. The resultant permutation code is used to encode the (ID/type/data) message. Then, an additional copy of the ID is also transmitted in the clear along with the encoded message. The format of the final packet to be transmitted becomes Packet= [ID{ID; type; datag}k] where {x}k constitutes encoding x with key k. Thus, instead of the traditional approach of sending the hash value (e.g., message digests and message authentication codes) along with the information to be sent, we use the result of the permutation code value locally. When the next node along the path to the sink receives the packet, it generates the local permutation
code to decode the packet.
 
Fig. 2. An illustration of the use of RC4 encryption mechanism in VEBEK.

Another significant step in the crypto module involves how the permutation code dictates the details of the encoding and decoding operations over the fields of the packet when generated by a source sensor or received by a forwarder sensor. Specifically, the permutation code P can be mapped to a set of actions to be taken on the data stream combination. As an example, the actions and their corresponding bit values can include simple operations such as shift, interleaving, taking the 1’s complement, etc. Other example operations can be seen in Table 2.
 
TABLE 2
Example Encoding Operations



 
Figure 3 Illustration of a sample encoding operation. (a) i + t + d bit string
before permutation. (b) Example encoding operations. (c) Example
permutation code value. (d) i + t + d bit string after permutation.
For example, if a node computed the following permutation code P={1100100101}, the string in Fig. 5a becomes the string in Fig. 3d before it is transmitted. The receiver will perform the same operations (since the inputs to RC4 are stored and updated on each sensor) to accurately decode the packet. To ensure correctness, the receiver compares the plaintext ID with the decoded ID. Moreover, although it is theoretically possible (1 in 2power(i+t+d) for a hacker to accurately inject data, it becomes increasingly unlikely as the packet grows. The benefits of this simple encoding scheme are: 1) since there is no hash code or message digest to transmit, the packet size does not grow, avoiding bandwidth overhead on an already resource-constrained network, thus increasing the network lifetime, 2) the technique is simple, thus
ideal for devices with limited resources (e.g., PDAs), and 3) the input to the RC4 encryption mechanism, namely, the key, changes dynamically without sending control messages to rekey.


NETWORKING MODULE:
Client-server computing or networking is a distributed application architecture that partitions tasks or workloads between service providers (servers) and service requesters, called clients. Often clients and servers operate over a computer network on separate hardware. A server machine is a high-performance host that is running one or more server programs which share its resources with clients. A client also shares any of its resources; Clients therefore initiate communication sessions with servers which await (listen to) incoming requests.
FORWARDING
The forwarding module is responsible for the sending of packets (reports) initiated at the current node (source node) or received packets from other sensors (forwarding nodes) along the path to the sink. The reports traverse the network through forwarding nodes and finally reach the terminating node, the sink. 
The final module in the VEBEK communication architecture is the forwarding module. The forwarding module is responsible for the sending of packets (reports) initiated at the current node (source node) or received packets from other sensors (forwarding nodes) along the path to the sink. The reports traverse the network through forwarding nodes and finally reach the terminating node, the sink. The operations of the forwarding module are explained in this section.
3.3.1 Source Node Algorithm
When an event is detected by a source node, the next step is for the report to be secured. The source node uses the local virtual energy value and an IV (or previous key value if not the first transmission) to construct the next key. As discussed earlier, this dynamic key generation process is primarily handled by the VEBEK module. The source sensor fetches the current value of the virtual energy from the VEBEK module. Then, the key is used as input into the RC4 algorithm inside the crypto module to create a permutation code for encoding the {ID/type/data} message. The encoded message and the cleartext ID of the originating node are transmitted to the next hop (forwarding node or sink) using the following format: [ID,{ID,type,data}Pc], where {x}basePc constitutes encoding x with permutation code Pc. The local virtual energy value is updated and stored for use with the transmission of the next report.
3.3.2 Forwarder Node Algorithm

Once the forwarding node receives the packet it will first check its watch-list to determine if the packet came from a node it is watching. If the node is not being watched by the current node, the packet is forwarded without modification or authentication. Although this node performed actions on the packet (received and forwarded the packet), its local virtual perceived energy value is not updated. This is done to maintain synchronization with nodes watching it further up the route. If the node is being watched by the current node, the forwarding node checks the associated current virtual energy record (Algorithm 2) stored for the sending node and extracts the energy value to derive the key. It then authenticates the message by decoding the message and comparing the plaintext node ID with the encoded node ID. If the packet is authentic, an updated virtual energy value is stored in the record associated with the sending node. If the packet is not authentic it is discarded. Again, the virtual energy value associated with the current sending node is only updated if this node has performed encoding on the packet.
Algorithm 2. 
Forwarding Node Algorithm with Communication Error Handling
1: Forwarder(currentNode,WatchedNode,UpstreamNode)
2: begin
3: i <-currentNode; enc <-0;WLi<- WatchList
4: k <-WatchedNode; src<- 0; j<- 0
5: Erxi , {IDclr, {msg}K} ReceivePacket()
6: if IDclr belongs WLi then
7: while (keyFound = 0)and(j <= threshold) do
8: Ekpi <- Fetch Virtual Energy(I, IDclr, enc, src)
9: K <-ComputeDynamicKey(Ekpi,IDclr)
10: Pc<- RC4(K,IDclr)
11: Edeci, MsgID decode(Pc, {msg}K)
12: if IDclr = MsgID then
13: keyFound<- true
14: else
15: j++
16: Ekpi<-Ekpi_ Etxi _ Eenci _ Erxi _ Edeci _ 2 _ Eai
17: end if
18: end while
19: if keyFound = true then
20: if j > 1 then
21: reEncode <- true
22: else
23: if Ebi > 0 then
24: reEncode <- true
25: else
26: reEncode <- false
27: end if
28: end if
29: if reEncode =true then
30: enc <- 1
31: Ebi <- Fetch Virtual Energy (I, IDclr, enc; src)
32: K <- ComputeDynamicKey(Ebi, IDclr)
33: Pc <- RC4(K, IDclr)
34: Eenci , {msg}Pc <- encode (Pc, msg)
35: packet <- (IDclr, {msg}Pc)
36: Etxi <- ForwardPacket()
37: Ebi <- Ebi _ Etxi _ Eenci _ Erxi _ Edeci _ 2 _ Eai
38: else
39: ForwardPacket() //Without any modification
40: end if
41: else
42: DropPacket() //Packet not valid
43: end if
44: else
45: ForwardPacket() //Without any modification
46: end if
47: end

Addressing Communication Errors via Virtual Bridge Energy
In VEBEK, to authenticate a packet, a node must keep track of the virtual energy of the sending node to derive the key needed for decoding. Ideally, once the authenticating node has the initial virtual energy value of the sending node, the value can be updated by decrementing the cost associated with the actions performed by the sending node using the cost equations defined in the previous sections on every successful packet reception. However, communication errors may cause some of the packets to be lost or dropped. Some errors may be due to the deployment region (e.g.,
underwater shadow zones) while operating on unreliable underlying protocols (e.g., medium access control protocol). For instance, ACK or data packets can be lost and the sender may not be able to determine which one actually was lost. Moreover, malicious packets inserted by attackers who impersonate legitimate sensors will be dropped intentionally by other legitimate sensors to filter the bad data out of the network. In such communication errors or intentional packet drop cases, the virtual energy value used to encode the next data packet at the sending node may differ from the virtual energy value that is stored for the sending node at its corresponding watching node. Specifically, the node that should have received the dropped packet and the nodes above that node on the path to the sink lose synchronization with the nodes below (because the upper portion never sees the lost packet and does not know to decrement the virtual energy associated with servicing the lost transmission). If another packet was to be forwarded by the current watching node using its current virtual energy, the upstream node(s) that watch this particular node would discard the packet. Thus, this situation needs to be resolved for proper functioning of the VEBEK framework.
To resolve potential loss of packets due to possible communication errors in the network, all the nodes are configured to store an additional virtual energy value, which we refer to as the Virtual Bridge Energy, Ebi , value to allow resynchronization (bridging) of the network at the next watching sensor node that determines that packets
were lost.
Definition 3 
Given a node, i, bridging is defined as the process of encoding the incoming packet coming from any sensor node in WLi for the upstream sensor node, j, with the key generated using the local copy of Ebi .
That is, as subsequent packets generated from the node of interest pass through the next watching node, the next watching node will decode the packet with the virtual
perceived energy key of the originating node and re encode the packet with the virtual bridge energy key, thus, the network will be kept synchronized. It is important to note
that once this value is activated for a watched node, it will be always used for packets coming from that node and used even if an error does not occur for the later transmissions of the same watched node. The watching node always updates and uses this parameter to keep the network bridged. Another pertinent point is the determination of packet loss by the first upstream watching node who will bridge the network. The VEBEK framework is designed to avoid extra messages and not increase the packet size to determine packet loss in the network. Thus, the next watching node tries to find the correct value of the virtual perceived energy for the key within a window of virtual energies. For this, a sensor is configured with a certain VirtualKeySearchThreshold value. That is, the watching node decrements the predefined virtual energy value from the current perceived energy at most virtualKeySearchThreshold times. When the node extracts the key successfully, it records the newest perceived energy value and associates it with the sender node (lines 7-18 in Algorithm 2). This approach may also be helpful in severe packet loss cases (i.e., bursty errors) by just properly configuring the virtualKeySearchThreshold value. However, if the watcher node exhausts all of the virtual energies within the threshold, it then classifies the packet as malicious. The combined use of virtual perceived and bridge energies assure the continued synchronization of the network as whole. The forwarding node algorithm including the
handling of communication errors is shown in Algorithm 2.



WSN SECURITY:
We focus on keying mechanisms for WSNs. There are two fundamental key management schemes for WSNs: static and dynamic. In static key management schemes, key management functions (i.e., key generation and distribution) are handled statically. That is, the sensors have a fixed number of keys loaded either prior to or shortly after network deployment. On the other hand, dynamic key management schemes perform keying functions (rekeying) either periodically or on demand as needed by the network. The sensors dynamically exchange keys to communicate. Although dynamic schemes are more attack resilient than static ones, one significant disadvantage is that they increase the communication overhead due to keys being refreshed or redistributed from time to time in the network. There are many reasons for key refreshment, including: updating keys after a key revocation has occurred, refreshing the key such that it does not become stale, or changing keys due to dynamic changes in the topology.
2.6 OPERATIONAL MODES (VEBEK):
The VEBEK protocol provides three security services authentication, integrity, and non repudiation. However, there are costs (communication, computation, and storage) associated providing these services. In reality, applications may have different security requirements. For instance, the security need of a military WSN application. The VEBEK framework also considers this need for flexibility and thus, supports two operational modes: VEBEK-I and VEBEK-II. The operational mode of VEBEK determines the number of nodes a particular sensor node must watch. Depending on the vigilance required inside the network, either of the operational modes can be configured for WSN applications.

VEBEK-I:

In the VEBEK-I operational mode, all nodes watch their neighbors; whenever a packet is received from a neighbor sensor node, it is decoded and its authenticity and integrity are verified. Only legitimate packets are forwarded toward the sink. In this mode, we assume there exists a short window of time at initial deployment that an adversary is not able to compromise the network, because it takes time for an attacker to capture a node or get keys. During this period, route initialization information may be used by each node to decide which node to watch and a record r is stored for each of its one-hop neighbors in its watch-list. To obtain a neighbor’s initial energy value, a network-wise master key can be used to transmit this value during this period similar to the shared-key discovery phase of other dynamic key management schemes. Alternatively, sensors can be preloaded with the initial energy value.

VEBEK-II:

In the VEBEK-II operational mode, nodes in the network are configured to only watch some of the nodes in the network. Each node randomly picks r nodes to monitor and stores the corresponding state before deployment. As a packet leaves the source node (originating node or forwarding node) it passes through node(s) that watch it probabilistically. Threshold before actually classifying the packet as malicious if the packet is authentic, and this hop is not the final destination, the original packet is forwarded unless the node is currently bridging the network. In the bridging case, the original packet is re encoded with the virtual bridge energy and forwarded. Since this node is bridging the network, both virtual and perceived energy values are decremented accordingly. If the packet is illegitimate, which is classified as such after exhausting all the virtual perceived energy values within the virtual Key Search Threshold window, the packet is discarded. This process continues until the packet reaches the sink.

Evaluation of the RC4 Algorithm for Data Encryption
Encryption is the process of transforming plaintext data into ciphertext in order to conceal its meaning and so preventing any unauthorized recipient from retrieving the original data. Hence, encryption is mainly used to ensure secrecy. Companies usually encrypt their data before transmission to ensure that the data is secure during transit. The encrypted data is sent over the public network and is decrypted by the intended recipient. Encryption works by running the data (represented as numbers) through a special encryption formula (called a key). Both the sender and the receiver know this key which may be used to encrypt and decrypt the data as shown in Fig.4.
 
Fig.4 Encryption/Decryption Block Diagram
Cryptography is a tool that can be used to keep information confidential and to ensure its integrity and authenticity. All modern cryptographic systems are based on Kerckhoff's principle of having a publicly-known algorithm and a secret key. Many cryptographic algorithms use complex transformations involving substitutions and permutations to transform the plaintext into the ciphertext. However, if quantum cryptography can be made practical, the use of one-time pads may provide truly unbreakable cryptosystems. Cryptographic algorithms can be divided into symmetric-key algorithms and public-key algorithms. Symmetric-key algorithms mangle the bits in a series of rounds parameterized by the key to turn the plaintext into the ciphertext. 
Triple DES and Rijndael (AES) are the most popular symmetric-key algorithms at present. These algorithms can be used in electronic code book mode, cipher block chaining mode, stream cipher mode, counter mode, and others Public-key algorithms have the property that different keys are used for encryption and decryption and that the decryption key cannot be derived from the encryption key. These properties make it possible to publish the public key. The main public-key algorithm is RSA, which derives its strength from the fact that it is very difficult to factor large numbers Legal, commercial, and other documents need to be signed. Accordingly, various schemes have been devised for digital signatures, using both symmetric-key and public-key algorithms. Commonly, messages to be signed are hashed using algorithms such as MD5 or SHA-1, and then the hashes are signed rather than the original messages. 
Public-key management can be done using certificates, which are documents that bind a principal to a public key. Certificates are signed by a trusted authority or by someone (recursively) approved by a trusted authority. The root of the chain has to be obtained in advance, but browsers generally have many root certificates built into them.
HOW ENCRYPTION WORKS
The encryption process involves taking each character of data and comparing it against a key. For example, one could encrypt the string “THE SKY IS HIGH” of data in any number of ways, for example, one may use a simple letter-number method. In this method, each letter in the alphabet corresponds to a particular number. If one uses a straight alphabetic to number encryption (i.e., A=1, B=2, C=3, and so on), this data is translated into the following numbers: 20 8 5 19 11 25 9 19 8 9 7 8. This series of numbers is then transmitted over a network, and the receiver can decrypt the string using the same key in reverse. From left to right, the number 20 translates to the letter T, 8 to H, 5 to E, and so on. Eventually, the receiver gets the entire message: “THE SKY IS HIGH”. Most encryption methods use much more complex formulas and methods. The sample key was about 8 bits long; some keys are extremely complex and can be as large as 128 bits. The larger the key (in bits), the more complex the encryption and the more difficult it is to be cracked.
2.1. Encryption Keys
To encode a message and decode an encrypted message, one needs the proper encryption key or keys. The encryption key is the table or formula that defines which character in the data translates to which encoded character. Here, encryption keys fall into two categories: public and private key encryption.
2.2. Private Key Encryption

Private keys are also known as symmetrical keys. In private key encryption technology, both the sender and receiver have the same key and use it to encrypt and decrypt all messages. This makes it difficult to initiate communication for the first time. How does one securely transmit the single key to each user? However, public keys encryption is used.
2.3. Public Key Encryption
Public key encryption, or a Diffie-Hellman algorithm, uses two keys to encrypt and decrypt data: a public key and a private key. Public keys are also known as asymmetrical keys. The receiver’s public key is used to encrypt a message then this message is sent to the receiver who can decrypt it using its own private key. This is a one-way communication. If the receiver wants to send a return message, the same principle is used. The message is encrypted with the original sender’s public key (the original sender is now going to be the receiver of this new message) and can only be decrypted with his or her private key. If the original sender does not have a public key, a message can still be sent with a digital certificate (also sometimes referred to as a digital ID). The digital ID verifies the sender of the message. Fig.5 shows public key– encrypted communication between two units, User X and User Y.

 
Fig.5 Public key encryption




3. METHODS OF ENCRYPTION
There are a variety of different types of encryption methods, they can be classified
according to the way in which the plaintext is processed (which can be either stream cipher or block cipher), or according to the type of operations used for transforming plaintext to ciphertext. The second class can be one of two styles, substitution (which maps each element in the plaintext into another element) and transposition (which rearranges elements in the plaintext).
Basically the two methods of producing ciphertext are stream cipher and block cipher. The two methods are similar except for the amount of data each encrypts on each pass. Most modern encryption schemes use some form of a block cipher.
3.1 Stream Cipher

Stream cipher is one of the simplest methods of encrypting data where each bit of the data is sequentially encrypted using one bit of the key as shown in Fig.6.
 
Fig.6 Stream ciphering and deciphering
In order to make a stream cipher more difficult to crack, one could use a crypto key which varies in length. This would help to mask any discernible patterns in the resulting ciphertext. In fact, by randomly changing the crypto key used on each bit of data, one can produce ciphertext that is mathematically impossible to crack. This is because using different random keys would not generate any repeating patterns which can give a cracker the clues required to break the crypto key. The main advantage of the stream cipher is that it is faster and more suitable for streaming application but its main disadvantage is that it is not suitable in some architecture. One example of the stream cipher method is the RC4 technique.

3.2 Block Cipher
Unlike stream ciphers, which encrypt every single bit, block ciphers are designed to encrypt data in chunks of a specific size as shown in Fig.7. A block cipher specification will identify how much data should be encrypted on each pass (called a block) as well as what size key should be applied to each block. For example, the Data Encryption Standard (DES) specifies that DES encrypted data should be processed in 64-bit blocks using a 56-bit key. There exist a number of different algorithms that can be used when processing block cipher encryption. The most basic is to simply take the data and break it up into blocks while applying the key to each block. While this method is efficient, it can produce repetitive ciphertext. If two blocks of data contain exactly the same information, the two resulting blocks of ciphertext will be identical, as well; a cracker can use ciphertext which repeats in a nonrandom fashion to break the crypto key. Blowfish encryption technique is an example of the block ciphering.
 
Fig.7 Block ciphering and deciphering
3.3 One Way Encryption

Another special type of encryption is the one way encryption, which is a method where the enciphering process is irreversible. The plaintext can never be recovered from the cipher text. This may seem pointless but it is probably the form of encryption that is the most familiar to computer users. Passwords on UNIX systems are encrypted by a one way algorithm. When a password is first chosen it is enciphered and placed into permanent storage. When the user logs on, the password entered at the login prompt is encrypted by the same algorithm and it is the resultant cipher text that is compared with the ciphertext held on disk. An encrypted password can only be broken by somebody correctly guessing the password, an important reason why passwords should be carefully chosen.
3.4 Hybrid Systems

By combining public and private key cryptosystems, it is possible to overcome some of the disadvantages of each. Public key pairs are used to set up a secure session, and then data is exchanged using a secret key system. This provides both the security and authentication processes associated with public key systems and the bulk data encryption capabilities of secret key systems. Pretty Good Privecy (PGP) is a well known security system used by computer enthusiasts to encrypt their email; it is an example of a practical hybrid encryption system which uses both secret key and public key.
4. RC4 ALGORITHM
RC4 is a stream cipher, symmetric key algorithm. The same algorithm is used for both
encryption and decryption as the data stream is simply XORed with the generated key
sequence. The key stream is completely independent of the plaintext used. It uses a variable length key from 1 to 256 bit to initialize a 256-bit state table. The state table is used for subsequent generation of pseudo-random bits and then to generate a pseudo-random stream which is XORed with the plaintext to give the ciphertext. The algorithm can be broken into two stages: initialization, and operation. In the initialization stage the 256-bit state table, S is populated, using the key, K as a seed. Once the state table is setup, it continues to be modified in a regular pattern as data is encrypted. The initialization process can be summarized by the pseudo-code;
j = 0;
for i = 0 to 255:
S[i] = i;
for i = 0 to 255:
j = (j + S[i] + K[i]) mod 256;
swap S[i] and S[j];
It is important to notice here the swapping of the locations of the numbers 0 to 255 (each
of which occurs only once) in the state table. The values of the state table are provided.
Once the initialization process is completed, the operation process may be summarized as
shown by the pseudo code below;
i = j = 0;
for (k = 0 to N-1) {
i = (i + 1) mod 256;
j = (j + S[i]) mod 256;
swap S[i] and S[j];
pr = S[ (S[i] + S[j]) mod 256]
output M[k] XOR pr
}
Where M[0..N-1] is the input message consisting of N bits.
This algorithm produces a stream of pseudo-random values. The input stream is XORed
with these values, bit by bit. The encryption and decryption process is the same as the data stream is simply XORed with the generated key sequence. If it is fed in an encrypted
message, it will produce the decrypted message output, and if it is fed in plaintext message, it will produce the encrypted version. The RC4 encryption algorithm is shown in Fig.8.
 
Fig.8 RC4 Encryption Algorithm
4.1. RC4 Steps
The steps for RC4 encryption algorithm is as follows:
1- Get the data to be encrypted and the selected key.
2- Create two string arrays.
3- Initiate one array with numbers from 0 to 255.
4- Fill the other array with the selected key.
5- Randomize the first array depending on the array of the key.
6- Randomize the first array within itself to generate the final key stream.
7- XOR the final key stream with the data to be encrypted to give cipher text.
Some of the RC4 algorithm features can be summarized as:
1- Symmetric stream cipher
2- Variable key length.
3- Very quick in software
4- Used for secured communications as in the encryption of traffic to and from secure
web sites using the SSL protocol.
The RC4 algorithm was also tested when sound data was encrypted and decrypted. A
typical wave file of size 136 KB is shown in Fig. 10. This file reads the words “one two three four five six” recorded by a male speaker, it was processed twelve times and the
encrypted/decrypted time was recorded as shown in Table 5 for each execution time. This
wave file was applied to the system and the encryption key length was allowed to vary. The best fitting curve which represents this behavior was obtained as shown by Eq. 1.

 
Table 3 example of a final key stream for RC4

y = -7E-06x6 + 0.0002x5 - 0.0027x4 + 0.0164x3 -0.0506x2 +0.077x +0.6707 (1)
Where (x) is the length of the encryption key in (characters) and (y) is the time for
encryption in seconds. The order of the polynomial could be chosen differently, but it was selected in a way such that the curve passes by the maximum number of points with the minimum possible order. The performance of the RC4 is tested here based on the processing time under certain conditions. However, this may vary according to the processor and the software used to implement the system. Hence, a comparison with another encryption algorithm may be used. The Blowfish encryption algorithm is referred to for this case . The two algorithms were executed under identical conditions. Accordingly, the encryption time for the varying size wave file for both RC4 and Blowfish encryption algorithms where the key length is kept fixed. It is clear that the two algorithms have almost the same performance in terms of execution time.
 
Table 4 Typical key length effect on encrypting a short data
 
Table 5 The processing time of a typical wave file in microseconds
 
Fig.9 Encryption time versus variable encryption key length for a small text data.
Utilizing these results, a module of the relationship between the file size and the
processing time can be illustrated as given by Eq.2 and Eq.3 for the RC4 and Blowfish
algorithms respectively.
y = 0.0003x2 - 0.0058x + 0.2621 (2)
y = 3E-06x3 - 0.0005x2 + 0.0594x - 1.4331 (3)
Where (x) is the size of the wave file to be encrypted in kilo byte (KB) and (y) is the time
for encryption in seconds.
 
Fig. 10 A typical wave file that is used.
The two equations were set to an order of 3 where this order was good enough
to produce a best fitting curve while simplifying the comparison between the two
algorithms.
y = -9E-08x3 +0.0018x2 -0.0645x +0.3796 (4)
y = -3E-07x3 +0.0019x2 -0.064x +0.3722 (5)
Where (x) is the size of the image file in Kilo bytes (KB) to be processed and (y) is the
execution time in seconds. Although the algorithm is not a data dependent, but the same exact equations could not be obtained for the wave and image data type, this is because the file size were not identical in both cases and so the executions time were not the same for the files which leads to different points to consider in best fitting curves. The variable key length and variable file size are also important in the decryption process. Simulation results obtained here for the relation between the variable key length and the decryption time for a sound data executed by both RC4 and Blowfish algorithms were obtained and modeled as given by Eq. 6 and Eq. 7 and that for the relation between the ciphertext file size and the decryption time is illustrated by Eq. 8 and Eq. 9 respectively.
y = 1E-06x6 -4E-05x5 +0.0005x4 -0.0031x3 +0.0088x2 -0.007x +0.7721 (6)
y = -8E-07x6 + 2E-05x5 - 0.0003x4 + 0.0013x3 - 0.0015x2 + 0.0009x + 0.7763 (7)
Where (x) is the length of the decryption key in characters and (y) is the time for decryption
in seconds.
y = 0.0003x2 - 0.0053x + 0.2669 (8)
y = 3E-06x3 - 0.0005x2 + 0.0603x - 1.4542 (9)
Where (x) is the size of the wave file to be decrypted in kilo byte (KB) and (y) is the time
for decryption in seconds.
The RC4 decryption time for an image file as a function of the key length is moduled as
given by Eq. 10 and that versus the file size is illustrated by Eq. 11.
y = -4E-07x3 + 0.0018x2 -0.0675x +0.4091 (10)
y = -8E-09x3 -0.0018x2 -0.0609x –0.3625 (11)

5 SDLC METHODOLOGIES
WATER FALL MODEL:
The model that is basically being followed is the WATER FALL MODEL, which states that the phases are organized in a linear order. First of all the feasibility study is done. Once that part is over the requirement analysis and project planning begins. If system exists one and modification and addition of new module is needed, analysis of present system can be used as basic model.
The design starts after the requirement analysis is complete and the coding begins after the design is complete. Once the programming is completed, the testing is done. In this model the sequence of activities performed in a software development project are: -
•	Requirement Analysis
•	Project Planning
•	System design
•	Detail design
•	Coding
•	Unit testing
•	System integration & testing
Here the linear ordering of these activities is critical. End of the phase and the output of one phase is the input of other phase. The output of each phase is to be consistent with the overall requirement of the system. Some of the qualities of spiral model are also incorporated like after the people concerned with the project review completion of each of the phase the work done.
             There are various software development approaches defined and designed which are used/employed during development process of software, these approaches are also referred as "Software Development Process Models". Each process model follows a particular life cycle in order to ensure success in process of software development.
One such approach/process used in Software Development is "The Waterfall Model". Waterfall approach was first Process Model to be introduced and followed widely in Software Engineering to ensure success of the project. In "The Waterfall" approach, the whole process of software development is divided into separate process phases. The phases in Waterfall model are: Requirement Specifications phase, Software Design, Implementation and Testing & Maintenance. All these phases are cascaded to each other so that second phase is started as and when defined set of goals are achieved for first phase and it is signed off, so the name "Waterfall Model". All the methods and processes undertaken in Waterfall Model are more visible.
The stages of "The Waterfall Model" are:
Requirement Analysis & Definition: All possible requirements of the system to be developed are captured in this phase. Requirements are set of functionalities and constraints that the end-user (who will be using the system) expects from the system. The requirements are gathered from the end-user by consultation, these requirements are analyzed for their validity and the possibility of incorporating the requirements in the system to be development is also studied. Finally, a Requirement Specification document is created which serves the purpose of guideline for the next phase of the model.
System & Software Design: Before a starting for actual coding, it is highly important to understand what we are going to create and what it should look like? The requirement specifications from first phase are studied in this phase and system design is prepared. System Design helps in specifying hardware and system requirements and also helps in defining overallsystem architecture. The system design specifications serve as input for the next phase of the model.
Implementation & Unit Testing: On receiving system design documents, the work is divided in modules/units and actual coding is started. The system is first developed in small programs called units, which are integrated in the next phase. Each unit is developed and tested for its functionality; this is referred to as Unit Testing. Unit testing mainly verifies if the modules/units meet their specifications.
Integration & System Testing: As specified above, the system is first divided in units which are developed and tested for their functionalities. These units are integrated into a complete system during Integration phase and tested to check if all modules/units coordinate between each other and the system as a whole behaves as per the specifications. After successfully testing the software, it is delivered to the customer.
Operations & Maintenance: This phase of "The Waterfall Model" is virtually never ending phase (Very long). Generally, problems with the system developed (which are not found during the development life cycle) come up after its practical use starts, so the issues related to the system are solved after deployment of the system. Not all the problems come in picture directly but they arise time to time and needs to be solved; hence this process is referred as Maintenance.
  There are some disadvantages of the Waterfall Model.
1) As it is very important to gather all possible requirements during the Requirement Gathering and Analysis phase in order to properly design the system, not all requirements are received at once, the requirements from customer goes on getting added to the list even after the end of "Requirement Gathering and Analysis" phase, this affects the system development process and its success in negative aspects.
2) The problems with one phase are never solved completely during that phase and in fact many problems regarding a particular phase arise after the phase is signed off, this results in badly structured system as not all the problems (related to a phase) are solved during the same phase.
3) The project is not partitioned in phases in flexible way.
4) As the requirements of the customer goes on getting added to the list, not all the requirements are fulfilled, this results in development of almost unusable system. These requirements are then met in newer version of the system; this increases the cost of system development.













Fig 11 : Water Fall Model
WATER FALL MODEL was being chosen because all requirements were known beforehand and the objective of our software development is the computerization/automation of an already existing manual working system

6 SYSTEM STUDY
FEASIBILITY STUDY
The feasibility of the project is analyzed in this phase and business proposal is put forth with a very general plan for the project and some cost estimates. During system analysis the feasibility study of the proposed system is to be carried out. This is to ensure that the proposed system is not a burden to the company.  For feasibility analysis, some understanding of the major requirements for the system is essential.
Three key considerations involved in the feasibility analysis are	
•	ECONOMICAL FEASIBILITY
•	TECHNICAL FEASIBILITY
•	SOCIAL FEASIBILITY

ECONOMICAL FEASIBILITY
                   This study is carried out to check the economic impact that the system will have on the organization. The amount of fund that the company can pour into the research and development of the system is limited. The expenditures must be justified. Thus the developed system as well within the budget and this was achieved because most of the technologies used are freely available. Only the customized products had to be purchased. 
TECHNICAL FEASIBILITY
                This study is carried out to check the technical feasibility, that is, the technical requirements of the system. Any system developed must not have a high demand on the available technical resources. This will lead to high demands on the available technical resources. This will lead to high demands being placed on the client. The developed system must have a modest requirement, as only minimal or null changes are required for implementing this system.   

SOCIAL FEASIBILITY
The aspect of study is to check the level of acceptance of the system by the user. This includes the process of training the user to use the system efficiently. The user must not feel threatened by the system, instead must accept it as a necessity. The level of acceptance by the users solely depends on the methods that are employed to educate the user about the system and to make him familiar with it. His level of confidence must be raised so that he is also able to make some constructive criticism, which is welcomed, as he is the final user of the system.

7 SYSTEM DESIGN
Systems design is the process of defining the architecture, components, modules, interfaces, and data for a system to satisfy specified requirements. The purpose of System Design is to create a technical solution that satisfies the functional requirements for the system.
5.1 DATA FLOW DIAGRAMS (DFD):
                The DFD is also called as bubble chart. It is a simple graphical formalism that can be used to represent a system in terms of the input data to the system, various processing carried out on these data, and the output data is generated by the system.
In the DFD, there are four symbols
1.	A square defines a source(originator) or destination of system data.
2.	An arrow identifies data flow.  It is the pipeline through which the information flows.
3.	A circle or a bubble represents a process that transforms incoming data flow into outgoing data flows.
4.	An open rectangle is a data store, data at rest or a temporary repository of data.





	Process that transforms data flow


  Source or Destination of data
	
Data flow

	Data Store

Constructing a DFD:
Several rules of thumb are used in drawing DFD’S:
1.	Process should be named and numbered for an easy reference.  Each name should be representative of the process.
2.	The direction of flow is from top to bottom and from left to right.  Data traditionally flow from source to the destination although they may flow back to the source.  One way to indicate this is to draw long flow line back to a source.  An alternative way is to repeat the source symbol as a destination.  Since it is used more than once in the DFD it is marked with a short diagonal.
3.	When a process is exploded into lower level details, they are numbered.
4.	The names of data stores and destinations are written in capital letters. Process and dataflow names have the first letter of each work capitalized.
A DFD typically shows the minimum contents of data store.  Each data store should contain all the data elements that flow in and out.
Questionnaires should contain all the data elements that flow in and out.  Missing interfaces redundancies and like is then accounted for often through interviews.
DATA FLOW DIAGRAM:
DFD:
 


SAILENT FEATURES OF DFD’S
1.	The DFD shows flow of data, not of control loops and decision are controlled considerations do not appear on a DFD.
2.	The DFD does not indicate the time factor involved in any process whether the dataflow take place daily, weekly, monthly or yearly.
3.	The sequence of events is not brought out on the DFD.

TYPES OF DATA FLOW DIAGRAMS
1.	Current Physical
2.	Current Logical
3.	New Logical
4.	New Physical
CURRENT PHYSICAL:
	In Current Physical DFD process label include the name of people or their positions or the names of computer systems that might provide some of the overall system-processing label includes an identification of the technology used to process the data.  Similarly data flows and data stores are often labels with the names of the actual physical media on which data are stored such as file folders, computer files, business forms or computer tapes.
CURRENT LOGICAL:
	The physical aspects at the system are removed as much as possible so that the current system is reduced to its essence to the data and the processors that transforms them regardless of actual physical form.
NEW LOGICAL:
	This is exactly like a current logical model if the user were completely happy with the user were completely happy with the functionality of the current system but had problems with how it was implemented typically through the new logical model will differ from current logical model while having additional functions, absolute function removal and inefficient flows recognized. 
NEW PHYSICAL:
The new physical represents only the physical implementation of the new system.


RULES GOVERNING THE DFD’S
PROCESS
1)	No process can have only outputs.
2)	No process can have only inputs.  If an object has only inputs than it must be a sink.
3)	A process has a verb phrase label.
DATA STORE
1)	Data cannot move directly from one data store to another data store, a process must move data.
2)	Data cannot move directly from an outside source to a data store, a process, which receives, must move data from the source and place the data into data store
3)	A data store has a noun phrase label.
SOURCE OR SINK
The origin and /or destination of data.
1)	Data cannot move direly from a source to sink it must be moved by a process
2)	A source and /or sink has a noun phrase land
DATA FLOW
1)	A Data Flow has only one direction of flow between symbols.  It may flow in both directions between a process and a data store to show a read before an update.  The later is usually indicated however by two separate arrows since these happen at different type.
2)	A join in DFD means that exactly the same data comes from any of two or more different processes data store or sink to a common location.
3)	A data flow cannot go directly back to the same process it leads.  There must be at least one other process that handles the data flow produce some other data flow returns the original data into the beginning process.
4)	A Data flow to a data store means update (delete or change).
5)	A data Flow from a data store means retrieve or use.
A data flow has a noun phrase label more than one data flow noun phrase can appear on a single arrow as long as all of the flows on the same arrow move together as one package.
5.2 UML DIAGRAM
                         The unified modeling language allows the software engineer to express an analysis model using the modeling notation that is governed by a set of syntactic semantic and pragmatic rules.
                    A UML system is represented using five different views that describe the system from distinctly different perspective. Each view is defined by a set of diagram, which is as follows.
User Model View
	This view represents the system from the users perspective.
	The analysis representation describes a usage scenario from the end-users perspective.
Structural model view
	In this model the data and functionality are arrived from inside the system.
	This model view models the static structures.
Behavioral Model View
	It represents the dynamic of behavioral as parts of the system, depicting the interactions of collection between various structural elements described in the user model and structural model view.

Implementation Model View
	In this the structural and behavioral as parts of the system are represented as they are to be built.
Environmental Model View
In this the structural and behavioral aspects of the environment in which the system is to be implemented are represented.
UML is specifically constructed through two different domains they are
•	UML Analysis modeling, which focuses on the user model and structural model views of the system?
•	UML design modeling, which focuses on the behavioral modeling, implementation modeling and environmental model views.
Use case Diagrams represent the functionality of the system from a user’s point of view. Use cases are used during requirements elicitation and analysis to represent the functionality of the system. Use cases focus on the behavior of the system from external point of view. 
Actors are external entities that interact with the system. Examples of actors include users like administrator, bank customer …etc., or another system like central database.
 
Class diagram:      A class diagram show’s set of classes interfaces collaboration’s and their relationships. Class diagram address the static design view of a system.
Object diagram: it shows set of objects and their relationship’s .it represents static instances of think found in class. These diagrams address the static design view of a system.
Use case diagram:   it shows set of use cases and actors and their relationships. Use case diagram address the static design view of a system.
Sequence diagram:  it is an interaction diagram that emphasizes the tine ordering messages.
Collaboration diagram:  it emphasizes the structural organization of objects that sender receive message.
Activity diagram: it is a special kind of State chart diagram that shows the flow from activity-activity within a system. It addresses the dynamic view of system.
CLASS DIAGRAM:
 
In software engineering, a class diagram in the Unified Modeling Language (UML) is a type of static structure diagram that describes the structure of a system by showing the system's classes, their attributes, operations (or methods), and the relationships among the classes.The class diagram is the main building block of object oriented modelling. It is used both for general conceptual modelling of the systematics of the application, and for detailed modelling translating the models into programming code. Class diagrams can also be used for data modeling.[1] The classes in a class diagram represent both the main objects and or interactions in the application and the objects to be programmed. In the class diagram these classes are represented with boxes which contain three parts:
•	The upper part holds the name of the class
•	The middle part contains the attributes of the class
•	The bottom part gives the methods or operations the class can take or undertake
In the system design of a system, a number of classes are identified and grouped together in a class diagram which helps to determine the static relations between those objects. With detailed modeling, the classes of the conceptual design are often split into a number of subclasses.
Class Diagram:
Policy Complaint:
 



Source:
 










Destination:
 






OBJECT DIGRAM:

 

An object diagram in the Unified Modeling Language (UML), is a diagram that shows a complete or partial view of the structure of a modeled system at a specific time.
An Object diagram focuses on some particular set of object instances and attributes, and the links between the instances. A correlated set of object diagrams provides insight into how an arbitrary view of a system is expected to evolve over time. Object diagrams are more concrete than class diagrams, and are often used to provide examples, or act as test cases for the class diagrams. Only those aspects of a model that are of current interest need be shown on an object diagram.

USECASE DIAGRAM:
 
In software and systems engineering, a use case (pronounced /juːs/, a case in the use of a system) is a list of steps, typically defining interactions between a role (known in UML as an "actor") and a system, to achieve a goal. The actor can be a human or an external system. In systems engineering, use cases are used at a higher level than within software engineering, often representing missions or stakeholder goals. The detailed requirements may then be captured in SysML or as contractual statements.
SEQUENCE DIAGRAM:


 
STATE DIGRAM:
 
The state diagram in the Unified Modeling Language is essentially a Harel statechart with standardized notation which can describe many systems, from computer programs to business processes. In UML 2 the name has been changed to State Machine Diagram. The following are the basic notational elements that can be used to make up a diagram:
•	Filled circle, pointing to the initial state
•	Hollow circle containing a smaller filled circle, indicating the final state (if any)
•	Rounded rectangle, denoting a state. Top of the rectangle contains a name of the state. Can contain a horizontal line in the middle, below which the activities that are done in that state are indicated
•	Arrow, denoting transition. The name of the event (if any) causing this transition labels the arrow body. A guard expression may be added before a "/" and enclosed in square-brackets ( eventName[guardExpression] ), denoting that this expression must be true for the transition to take place. If an action is performed during this transition, it is added to the label following a "/" ( eventName[guardExpression]/action ).
•	Thick horizontal line with either x>1 lines entering and 1 line leaving or 1 line entering and x>1 lines leaving. These denote join/fork, respectively.
COLLOBARATION DIAGRAM:
 
A Collaboration Diagram is a diagram that shows object interactions organized around the objects and their links to each other. Unlike a Sequence Diagram, a Collaboration Diagram shows the relationships among the objects. Sequence diagrams and collaboration diagrams express similar information, but show it in different ways.


6 SOFTWARE ENVIRONMENT
Features Of .Net
Microsoft .NET is a set of Microsoft software technologies for rapidly building and integrating XML Web services, Microsoft Windows-based applications, and Web solutions. The .NET Framework is a language-neutral platform for writing programs that can easily and securely interoperate. There’s no language barrier with .NET: there are numerous languages available to the developer including Managed C++, C#, Visual Basic and Java Script. The .NET framework provides the foundation for components to interact seamlessly, whether locally or remotely on different platforms. It standardizes common data types and communications protocols so that components created in different languages can easily interoperate.

 “.NET” is also the collective name given to various software components built upon the .NET platform. These will be both products (Visual Studio.NET and Windows.NET Server, for instance) and services (like Passport, .NET My Services, and so on).
THE .NET FRAMEWORK
The .NET Framework has two main parts:
1. The Common Language Runtime (CLR).
2. A hierarchical set of class libraries.
The CLR is described as the “execution engine” of .NET. It provides the environment within which programs run. The most important features are:
•	Conversion from a low-level assembler-style language, called Intermediate Language (IL), into code native to the platform being executed on.
•	Memory management, notably including garbage collection.
•	Checking and enforcing security restrictions on the running code.
•	Loading and executing programs, with version control and other such features.
The following features of the .NET framework are also worth description
Managed Code 
The code that targets .NET, and which contains certain extra Information - “metadata” - to describe itself. Whilst both managed and unmanaged code can run in the runtime, only managed code contains the information that allows the CLR to guarantee, for instance, safe execution and interoperability.
Managed Data 
 With Managed Code comes Managed Data. CLR provides memory allocation and Deal location facilities, and garbage collection. Some .NET languages use Managed Data by default, such as C#, Visual Basic.NET and JScript.NET, whereas others, namely C++, do not. Targeting CLR can, depending on the language you’re using, impose certain constraints on the features available. As with managed and unmanaged code, one can have both managed and unmanaged data in .NET applications - data that doesn’t get garbage collected but instead is looked after by unmanaged code.
Common Type System 
The CLR uses something called the Common Type System (CTS) to strictly enforce type-safety. This ensures that all classes are compatible with each other, by describing types in a common way. CTS define how types work within the runtime, which enables types in one language to interoperate with types in another language, including cross-language exception handling. As well as ensuring that types are only used in appropriate ways, the runtime also ensures that code doesn’t attempt to access memory that hasn’t been allocated to it.
Common Language Specification 
 The CLR provides built-in support for language interoperability. To ensure that you can develop managed code that can be fully used by developers using any programming language, a set of language features and rules for using them called the Common Language Specification (CLS) has been defined. Components that follow these rules and expose only CLS features are considered CLS-compliant.
THE CLASS LIBRARY
.NET provides a single-rooted hierarchy of classes, containing over 7000 types. The root of the namespace is called System; this contains basic types like Byte, Double, Boolean, and String, as well as Object. All objects derive from System. Object. As well as objects, there are value types. Value types can be allocated on the stack, which can provide useful flexibility. There are also efficient means of converting value types to object types if and when necessary.
The set of classes is pretty comprehensive, providing collections, file, screen, and network I/O, threading, and so on, as well as XML and database connectivity.
The class library is subdivided into a number of sets (or namespaces), each providing distinct areas of functionality, with dependencies between the namespaces kept to a minimum. The second most important piece of the .NET Framework is the .NET Framework class library (FCL). As you've seen, the common language runtime handles the dirty work of actually running the code you write. But to write the code, you need a foundation of available classes to access the resources of the operating system, database server, or file server. The FCL is made up of a hierarchy of namespaces that expose classes, structures, interfaces, enumerations, and delegates that give you access to these resources.
The namespaces are logically defined by functionality. For example, the System.Data namespace contains all the functionality available to accessing databases. This namespace is further broken down into System.Data.SqlClient, which exposes functionality specific to SQL Server, and System.Data.OleDb, which exposes specific functionality for accessing OLEDB data sources. The bounds of a namespace aren't necessarily defined by specific assemblies within the FCL; rather, they're focused on functionality and logical grouping. In total, there are more than 20,000 classes in the FCL, all logically grouped in a hierarchical manner. Figure 1.8 shows where the FCL fits into the .NET Framework and the logical grouping of namespaces.

 
Figure 1.8. The .NET Framework class library.

To use an FCL class in your application, you use the Imports statement in Visual Basic .NET or the using statement in C#. When you reference a namespace in Visual Basic .NET or C#, you also get the convenience of auto-complete and auto-list members when you access the objects' types using Visual Studio .NET. This makes it very easy to determine what types are available for each class in the namespace you're using. As you'll see over the next several weeks, it's very easy to start coding in Visual Studio .NET.
The Structure of a .NET Application
To understand how the common language runtime manages code execution, you must examine the structure of a .NET application. The primary unit of a .NET application is the assembly. An assembly is a self-describing collection of code, resources, and metadata. The assembly manifest contains information about what is contained within the assembly. The assembly manifest provides:
•	Identity information, such as the assembly’s name and version number
•	A list of all types exposed by the assembly
•	A list of other assemblies required by the assembly
•	A list of code access security instructions, including permissions required by the assembly and permissions to be denied the assembly
    Each assembly has one and only one assembly manifest, and it contains all the description information for the assembly. However, the assembly manifest can be contained in its own file or within one of the assembly’s modules.
An assembly contains one or more modules. A module contains the code that makes up your application or library, and it contains metadata that describes that code. When you compile a project into an assembly, your code is converted from high-level code to IL. Because all managed code is first converted to IL code, applications written in different languages can easily interact. For example, one developer might write an application in Visual C# that accesses a DLL in Visual Basic .NET. Both resources will be converted to IL modules before being executed, thus avoiding any language-incompatibility issues.
Each module also contains a number of types. Types are templates that describe a set of data encapsulation and functionality. There are two kinds of types: reference types (classes) and value types (structures). These types are discussed in greater detail in Lesson 2 of this chapter. Each type is described to the common language runtime in the assembly manifest. A type can contain fields, properties, and methods, each of which should be related to a common functionality. 
For example, you might have a class that represents a bank account. It contains fields, properties, and methods related to the functions needed to implement a bank account. A field represents storage of a particular type of data. One field might store the name of an account holder, for example. Properties are similar to fields, but properties usually provide some kind of validation when data is set or retrieved. You might have a property that represents an account balance.
 When an attempt is made to change the value, the property can check to see if the attempted change is greater than a predetermined limit. If the value is greater than the limit, the property does not allow the change. Methods represent behavior, such as actions taken on data stored within the class or changes to the user interface. Continuing with the bank account example, you might have a Transfer method that transfers a balance from a checking account to a savings account, or an Alert method that warns users when their balances fall below a predetermined level.
Compilation and Execution of a .NET Application
When you compile a .NET application, it is not compiled to binary machine code; rather, it is converted to IL. This is the form that your deployed application takes—one or more assemblies consisting of executable files and DLL files in IL form. At least one of these assemblies will contain an executable file that has been designated as the entry point for the application.
When execution of your program begins, the first assembly is loaded into memory. At this point, the common language runtime examines the assembly manifest and determines the requirements to run the program. It examines security permissions requested by the assembly and compares them with the system’s security policy. If the system’s security policy does not allow the requested permissions, the application will not run. If the application passes the system’s security policy, the common ¬language runtime executes the code. It creates a process for the application to run in and begins application execution. 
When execution starts, the first bit of code that needs to be executed is loaded into memory and compiled into native binary code from IL by the common language runtime’s Just-In-Time (JIT) compiler. Once compiled, the code is executed and stored in memory as native code. Thus, each portion of code is compiled only once when an application executes. Whenever program execution branches to code that has not yet run, the JIT compiler compiles it ahead of execution and stores it in memory as binary code.
This way, application performance is maximized because only the parts of a program that are executed are compiled.
The .NET Framework base class library contains the base classes that provide many of the services and objects you need when writing your applications. The class library is organized into namespaces. A namespace is a logical grouping of types that perform related functions. For example, the System.Windows.Forms namespace contains all the types that make up Windows forms and the controls used in those forms.
Namespaces are logical groupings of related classes. The namespaces in the .NET base class library are organized hierarchically. The root of the .NET Framework is the System namespace. Other namespaces can be accessed with the period operator. A typical namespace construction appears as follows:
System
System.Data
System.Data.SQLClient
The first example refers to the System namespace. The second refers to the System.Data namespace. The third example refers to the System.Data.SQLClient namespace. 
Table 1.1 introduces some of the more commonly used .NET base class namespaces.
Table 1-1. Representative .NET Namespaces 
Namespace	Description
System	This namespace is the root for many of the low-level types required by the .NET Framework. It is the root for primitive data types as well, and it is the root for all the other namespaces in the .NET base class library.
System.Collections	This namespace contains classes that represent a variety of different container types, such as ArrayList, SortedList, Queue, and Stack. You also can find abstract classes, such as CollectionBase, which are useful for implementing your own collection functionality.
System.ComponentModel	This namespace contains classes involved in component creation and containment, such as attributes, type converters, and license providers.
System.Data	This namespace contains classes required for database access and manipulations, as well as additional namespaces used for data access.
System.Data.Common	This namespace contains a set of classes that are shared by the .NET managed data providers.
System.Data.OleDb	This namespace contains classes that make up the managed data provider for OLE DB data access.
System.Data.SQLClient	This namespace contains classes that are optimized for interacting with Microsoft SQL Server.
System.Drawing	This namespace exposes GDI+ functionality and provides classes that facilitate graphics rendering.
System.IO	In this namespace, you will find types for handling file system I/O.
System.Math	This namespace is home to common mathematics functions such as extracting roots and trigonometry.
System.Reflection	This namespace provides support for obtaining information and dynamic creation of types at runtime.
System.Security	This namespace is home to types dealing with permissions, cryptography, and code access security.
System.Threading	This namespace contains classes that facilitate the implementation of multithreaded applications.
System.Windows.Forms	This namespace contains types involved in creating standard Windows applications. Classes that represent forms and controls reside here as well.
The namespace names are self-descriptive by design. Straightforward names make the .NET Framework easy to use and allow you to rapidly familiarize yourself with its contents.
LANGUAGES SUPPORTED BY .NET
The multi-language capability of the .NET Framework and Visual Studio .NET enables developers to use their existing programming skills to build all types of applications and XML Web services. The .NET framework supports new versions of Microsoft’s old favorites Visual Basic and C++ (as VB.NET and Managed C++), but there are also a number of new additions to the family.
Visual Basic .NET has been updated to include many new and improved language features that make it a powerful object-oriented programming language. These features include inheritance, interfaces, and overloading, among others. Visual Basic also now supports structured exception handling, custom attributes and also supports multi-threading. 
Visual Basic .NET is also CLS compliant, which means that any CLS-compliant language can use the classes, objects, and components you create in Visual Basic .NET.
Managed Extensions for C++ and attributed programming are just some of the enhancements made to the C++ language. Managed Extensions simplify the task of migrating existing C++ applications to the new .NET Framework.
C# is Microsoft’s new language. It’s a C-style language that is essentially “C++ for Rapid Application Development”. Unlike other languages, its specification is just the grammar of the language. It has no standard library of its own, and instead has been designed with the intention of using the .NET libraries as its own. 
Microsoft Visual J# .NET provides the easiest transition for Java-language developers into the world of XML Web Services and dramatically improves the interoperability of Java-language programs with existing software written in a variety of other programming languages. 
Active State has created Visual Perl and Visual Python, which enable .NET-aware applications to be built in either Perl or Python. Both products can be integrated into the Visual Studio .NET environment. Visual Perl includes support for Active State’s Perl Dev Kit.
Other languages for which .NET compilers are available include
•	FORTRAN
•	COBOL
•	Eiffel           

            ASP.NET
XML WEB SERVICES	   Windows Forms
                         Base Class Libraries
                   Common Language Runtime
                           Operating System

Fig1 .Net Framework
C#.NET is also compliant with CLS (Common Language Specification) and supports structured exception handling. CLS is set of rules and constructs that are supported by the CLR (Common Language Runtime). CLR is the runtime environment provided by the .NET Framework; it manages the execution of the code and also makes the development process easier by providing services.    
C#.NET is a CLS-compliant language. Any objects, classes, or components that created in C#.NET can be used in any other CLS-compliant language. In addition, we can use objects, classes, and components created in other CLS-compliant languages in C#.NET .The use of CLS ensures complete interoperability among applications, regardless of the languages used to create the application.
CONSTRUCTORS AND DESTRUCTORS:
Constructors are used to initialize objects, whereas destructors are used to destroy them. In other words, destructors are used to release the resources allocated to the object. In C#.NET the sub finalize procedure is available. The sub finalize procedure is used to complete the tasks that must be performed when an object is destroyed. The sub finalize procedure is called automatically when an object is destroyed. In addition, the sub finalize procedure can be called only from the class it belongs to or from derived classes.
GARBAGE COLLECTION
Garbage Collection is another new feature in C#.NET. The .NET Framework monitors allocated resources, such as objects and variables. In addition, the .NET Framework automatically releases memory for reuse by destroying objects that are no longer in use. 
In C#.NET, the garbage collector checks for the objects that are not currently in use by applications. When the garbage collector comes across an object that is marked for garbage collection, it releases the memory occupied by the object.
OVERLOADING
Overloading is another feature in C#. Overloading enables us to define multiple procedures with the same name, where each procedure has a different set of arguments. Besides using overloading for procedures, we can use it for constructors and properties in a class.
MULTITHREADING:
C#.NET also supports multithreading. An application that supports multithreading can handle multiple tasks simultaneously, we can use multithreading to decrease the time taken by an application to respond to user interaction. 
STRUCTURED EXCEPTION HANDLING
C#.NET supports structured handling, which enables us to detect and remove errors at runtime. In C#.NET, we need to use Try…Catch…Finally statements to create exception handlers. Using Try…Catch…Finally statements, we can create robust and effective exception handlers to improve the performance of our application.
THE .NET FRAMEWORK
The .NET Framework is a new computing platform that simplifies application development in the highly distributed environment of the Internet.
 OBJECTIVES OF. NET FRAMEWORK
1. To provide a consistent object-oriented programming environment whether object codes is stored and executed locally on Internet-distributed, or executed remotely.
2. To provide a code-execution environment to minimizes software deployment and guarantees safe execution of code.
3. Eliminates the performance problems.          
There are different types of application, such as Windows-based applications and Web-based applications.  
Features of SQL-SERVER
The OLAP Services feature available in SQL Server version 7.0 is now called SQL Server 2000 Analysis Services. The term OLAP Services has been replaced with the term Analysis Services. Analysis Services also includes a new data mining component. The Repository component available in SQL Server version 7.0 is now called Microsoft SQL Server 2000 Meta Data Services. References to the component now use the term Meta Data Services. The term repository is used only in reference to the repository engine within Meta Data Services
SQL-SERVER database consist of six type of objects,
They are,
1. TABLE
2. QUERY
3. FORM
4. REPORT
5. MACRO
TABLE:
A database is a collection of data about a specific topic.
VIEWS OF TABLE:
We can work with a table in two types,
1. Design View
2. Datasheet View
Design View
To build or modify the structure of a table we work in the table design view. We can specify what kind of data will be hold.
Datasheet View
To add, edit or analyses the data itself we work in tables datasheet view mode.
QUERY:
  A query is a question that has to be asked the data. Access gathers data that answers the question from one or more table. The data that make up the answer is either dynaset (if you edit it) or a snapshot (it cannot be edited).Each time we run query, we get latest information in the dynaset. Access either displays the dynaset or snapshot for us to view or perform an action on it, such as deleting or updating.
FORMS:
           A form is used to view and edit information in the database record by record .A form displays only the information we want to see in the way we want to see it. Forms use the familiar controls such as textboxes and checkboxes. This makes viewing and entering data easy.
Views of Form:
We can work with forms in several primarily there are two views, They are,
1. Design View 
2. Form View
Design View  
To build or modify the structure of a form, we work in forms design view. We can add control to the form that are bound to fields in a table or query, includes textboxes, option buttons, graphs and pictures.
Form View
The form view which display the whole design of the form.
REPORT:
A report is used to vies and print information from the database. The report can ground records into many levels and compute totals and average by checking values from many records at once. Also the report is attractive and distinctive because we have control over the size and appearance of it.
MACRO :
A macro is a set of actions. Each action in macros does something. Such as opening a form or printing a report .We write macros to automate the common tasks the work easy and save the time.


.NET APPLICATIONS
1)Console applications
2) Windows GUI applications (Windows Forms)
3) Windows Presentation Foundation (WPF) applications
4) ASP.Net applications (Web Applications)
5) Web services applications
6) Windows services applications
CONSOLE APPLICATION
A console application is a computer program designed to be used via a text-only computer interface, such as a text terminal, the command line interface of some operating systems (Unix, DOS, etc.) or the text-based interface included with most Graphical User Interface (GUI) operating systems, such as the Win32 console in Microsoft Windows, the Terminal in Mac OS X, and xterm in Unix. A user typically interacts with a console application using only a keyboard and display screen, as opposed to GUI applications, which normally require the use of a mouse or other pointing device. Many console applications such as command line interpreters are command line tools, but numerous Text User Interface (TUI) programs also exist.
Windows GUI applications (Windows Forms)
Windows Forms (WinForms) is the name given to the graphical application programming interface (API) included as a part of Microsoft .NET Framework, providing access to native Microsoft Windows interface elements by wrapping the extant Windows API in managed code.
Windows Presentation Foundation (WPF) applications
Windows Presentation Foundation (or WPF) is a computer-software graphical subsystem for rendering user interfaces in Windows-based applications. WPF, previously known as "Avalon", was initially released as part of .NET Framework 3.0. Rather than relying on the older GDI subsystem, WPF utilizes DirectX. WPF attempts to provide a consistent programming model for building applications and provides a separation between the user interface and the business logic. It resembles similar XML-oriented object models, such as those implemented in XUL and SVG.

ASP.Net applications (Web Applications)
ASP.NET is a Web application framework developed and marketed by Microsoft to allow programmers to build dynamic Web sites, Web applications and Web services. It was first released in January 2002 with version 1.0 of the .NET Framework, and is the successor to Microsoft's Active Server Pages (ASP) technology. ASP.NET is built on the Common Language Runtime (CLR), allowing programmers to write ASP.NET code using any supported .NET language. The ASP.NET SOAP extension framework allows ASP.NET components to process SOAP messages.
Web services applications
A Web service is a method of communication between two electronic devices over the web (internet). The W3C defines a "Web service" as "a software system designed to support interoperable machine-to-machine interaction over a network". It has an interface described in a machine-processable format (specifically Web Services Description Language, known by the acronym WSDL). Other systems interact with the Web service in a manner prescribed by its description using SOAP messages, typically conveyed using HTTP with an XML serialization in conjunction with other Web-related standards.
Windows services applications
Windows service is a long-running executable that performs specific functions and which is designed not to require user intervention. Windows services can be configured to start when the operating system is booted and run in the background as long as Windows is running, or they can be started manually when required. They are similar in concept to a Unix daemon. Many appear in the processes list in the Windows Task Manager, most often with a username of SYSTEM, LOCAL SERVICE or NETWORK SERVICE, though not all processes with the SYSTEM username are services. The remaining services run through svchost.exe as DLLs loaded into memory.
7 TECHNOLOGY DESCRIPTION
INPUT  AND OUPUT DESIGN :
The input design is the link between the information system and the user. It comprises the developing specification and procedures for data preparation and those steps are necessary to put transaction data in to a usable form for processing can be achieved by inspecting the computer to read data from a written or printed document or it can occur by having people keying the data directly into the system. The design of input focuses on controlling the amount of input required, controlling the errors, avoiding delay, avoiding extra steps and keeping the process simple. The input is designed in such a way so that it provides security and ease of use with retaining the privacy. Input Design considered the following things:
	What data should be given as input?
	 How the data should be arranged or coded?
	 The dialog to guide the operating personnel in providing input.
	Methods for preparing input validations and steps to follow when error occur.

OBJECTIVES
1.Input Design is the process of converting a user-oriented description of the input into a computer-based system. This design is important to avoid errors in the data input process and show the correct direction to the management for getting correct information from the computerized system.
2. It is achieved by creating user-friendly screens for the data entry to handle large volume of data. The goal of designing input is to make data entry easier and to be free from errors. The data entry screen is designed in such a way that all the data manipulates can be performed. It also provides record viewing facilities.
3.When the data is entered it will check for its validity. Data can be entered with the help of screens. Appropriate messages are provided as when needed so that the user
 will not be in maize of instant. Thus the objective of input design is to create an input layout that is easy to follow
OUTPUT DESIGN
A quality output is one, which meets the requirements of the end user and presents the information clearly. In any system results of processing are communicated to the users and to other system through outputs. In output design it is determined how the information is to be displaced for immediate need and also the hard copy output. It is the most important and direct source information to the user. Efficient and intelligent output design improves the system’s relationship to help user decision-making.
1. Designing computer output should proceed in an organized, well thought out manner; the right output must be developed while ensuring that each output element is designed so that people will find the system can use easily and effectively. When analysis design computer output, they should Identify the specific output that is needed to meet the requirements.
2.Select methods for presenting information.
3.Create document, report, or other formats that contain information produced by the system.
The output form of an information system should accomplish one or more of the following objectives.
	Convey information about past activities, current status or projections of the
	Future.
	Signal important events, opportunities, problems, or warnings.
	Trigger an action.
	Confirm an action.

8 SYSTEM TESTING AND DEBUGGING
SYSTEM TESTING
                     The purpose of testing is to discover errors. Testing is the process of trying to discover every conceivable fault or weakness in a work product. It provides a way to check the functionality of components, sub assemblies, assemblies and/or a finished product It is the process of exercising software with the intent of ensuring that the
Software system meets its requirements and user expectations and does not fail in an unacceptable manner. There are various types of test. Each test type addresses a specific testing requirement.
TYPES OF TESTS
Testing:
Introduction:
After finishing the development of any computer based system the next complicated time consuming process is system testing. During the time of testing only the development company can know that, how far the user requirements have been met out, and so on.
	Software testing is an important element of the software quality assurance and represents the ultimate review of specification, design and coding. The increasing feasibility of software as a system and the cost associated with the software failures are motivated forces for well planned through testing.
Testing Objectives
       These are several rules that can save as testing objectives they are:      
	Testing is a process of executing program with the intent of finding an error.
	A good test case is one that has a high probability of finding an undiscovered error.
Testing procedures for the project is done in the following sequence
	System testing is done for checking the server name of the machines being connected between the customer and executive.
	The product information provided by the company to the executive is tested against the validation with the centralized data store. 
	System testing is also done for checking the executive availability to connected to the server.
	The server name authentication is checked and availability to the customer 
	Proper communication chat line viability is tested and made the chat system function properly.
	Mail functions are tested against the user concurrency and customer mail 
date validate.
Following are the some of the testing methods applied to this effective project:
SOURCE CODE TESTING:
This examines the logic of the system. If we are getting the output that is required by the user, then we can say that the logic is perfect.
SPECIFICATION TESTING:
We can set with, what program should do and how it should perform under various condition. This testing is a comparative study of evolution of system performance and system requirements.
MODULE LEVEL TESTING:
In this the error will be found at each individual module, it encourages the programmer to find and rectify the errors without affecting the other modules


Unit testing
          Unit testing involves the design of test cases that validate that the internal program logic is functioning properly, and that program inputs produce valid outputs. All decision branches and internal code flow should be validated. It is the testing of individual software units of the application .it is done after the completion of an individual unit before integration. This is a structural testing, that relies on knowledge of its construction and is invasive. Unit tests perform basic tests at component level and test a specific business process, application, and/or system configuration. Unit tests ensure that each unique path of a business process performs accurately to the documented specifications and contains clearly defined inputs and expected results.
Integration testing
 Integration tests are designed to test integrated software components to determine if they actually run as one program.  Testing is event driven and is more concerned with the basic outcome of screens or fields. Integration tests demonstrate that although the components were individually satisfaction, as shown by successfully unit testing, the combination of components is correct and consistent. Integration testing is specifically aimed at   exposing the problems that arise from the combination of components.
Functional test
 Functional tests provide systematic demonstrations that functions tested are available as specified by the business and technical requirements, system documentation, and user manuals.
Functional testing is centered on the following items:
Valid Input               :  identified classes of valid input must be accepted.
Invalid Input             : identified classes of invalid input must be rejected.
Functions                  : identified functions must be exercised.
Output           	   : identified classes of application outputs must be exercised.
Systems/Procedures: interfacing systems or procedures must be invoked.
Organization and preparation of functional tests is focused on requirements, key functions, or special test cases. In addition, systematic coverage pertaining to identify Business process flows; data fields, predefined processes, and successive processes must be considered for testing. Before functional testing is complete, additional tests are identified and the effective value of current tests is determined.
System Test
    System testing ensures that the entire integrated software system meets requirements. It tests a configuration to ensure known and predictable results. An example of system testing is the configuration oriented system integration test. System testing is based on process descriptions and flows, emphasizing pre-driven process links and integration points.

White Box Testing
        White Box Testing is a testing in which in which the software tester has knowledge of the inner workings, structure and language of the software, or at least its purpose. It is purpose. It is used to test areas that cannot be reached from a black box level.
Black Box Testing
        Black Box Testing is testing the software without any knowledge of the inner workings, structure or language of the module being tested. Black box tests, as most other kinds of tests, must be written from a definitive source document, such as specification or requirements document, such as specification or requirements document. It is a testing in which the software under test is treated, as a black box .you cannot “see” into it. The test provides inputs and responds to outputs without considering how the software works.
Unit Testing:Unit testing is usually conducted as part of a combined code and unit test phase of the software lifecycle, although it is not uncommon for coding and unit testing to be conducted as two distinct phases.

Test strategy and approach
	Field testing will be performed manually and functional tests will be written in detail.

Test objectives
•	All field entries must work properly.
•	Pages must be activated from the identified link.
•	The entry screen, messages and responses must not be delayed.
Features to be tested
•	Verify that the entries are of the correct format
•	No duplicate entries should be allowed
•	All links should take the user to the correct page.

Integration Testing
Software integration testing is the incremental integration testing of two or more integrated software components on a single platform to produce failures caused by interface defects.
	The task of the integration test is to check that components or software applications, e.g. components in a software system or – one step up – software applications at the company level – interact without error.
Test Results: All the test cases mentioned above passed successfully. No defects encountered.
Acceptance Testing
User Acceptance Testing is a critical phase of any project and requires significant participation by the end user. It also ensures that the system meets the functional requirements.
Test Results: All the test cases mentioned above passed successfully. No defects encountered.
9 SCREEN SHOTS

 
 
 
 
 
 
 
 
 










10 CONCLUSION
Communication is very costly for wireless sensor networks WSNs) and for certain WSN applications. Independent of the goal of saving energy, it may be very important to minimize the exchange of messages (e.g., military scenarios). To address these concerns, we presented a secure communication framework for WSNs called Virtual Energy- Based Encryption and Keying. 
We have evaluated VEBEK’s feasibility and performance through both theoretical analysis and simulations. Our results show that different operational modes of VEBEK can be configured to provide optimal performance in a variety of network configurations depending largely on the application of the sensor network. We also compared the energy performance of our framework with other en route malicious data filtering schemes. Our results show that VEBEK performs better than others while providing support for communication error handling, which was not the focus of earlier studies. 










REFERENCES
[1] I.F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci, “Wireless Sensor Networks: A Survey,” Computer Networks, vol. 38, no. 4, pp. 393-422, Mar. 2002.
[2] C. Vu, R. Beyah, and Y. Li, “A Composite Event Detection in Wireless Sensor Networks,” Proc. IEEE Int’l Performance, Computing, and Comm. Conf. (IPCCC ’07), Apr. 2007.
[3] S. Uluagac, C. Lee, R. Beyah, and J. Copeland, “Designing Secure Protocols for Wireless Sensor Networks,” Wireless Algorithms, Systems, and Applications, vol. 5258, pp. 503-514, Springer, 2008.
[4] Crossbow Technology, http://www.xbow.com, 2008. [5] G.J. Pottie and W.J. Kaiser, “Wireless Integrated Network Sensors,” Comm. ACM, vol. 43, no. 5, pp. 51-58, 2000.
[5] R. Roman, C. Alcaraz, and J. Lopez, “A Survey of Cryptographic Primitives and Implementations for Hardware-Constrained Sensor Network Nodes,” Mobile Networks and Applications, vol. 12, no. 4, pp. 231-244, Aug. 2007.
[6] H. Hou, C. Corbett, Y. Li, and R. Beyah, “Dynamic Energy-Based Encoding and Filtering in Sensor Networks,” Proc. IEEE Military Comm. Conf. (MILCOM ’07), Oct. 2007.
[7] L. Eschenauer and V.D. Gligor, “A Key-Management Scheme for Distributed Sensor Networks,” Proc. Ninth ACM Conf. Computer and Comm. Security, pp. 41-4, 2002.
[8] M. Eltoweissy, M. Moharrum, and R. Mukkamala, “Dynamic Key Management in Sensor Networks,” IEEE Comm. Magazine, vol. 44, no. 4, pp. 122-130, Apr. 2006.
[9] M. Zorzi and R. Rao, “Geographic Random Forwarding (GeRaF) for Ad Hoc and Sensor Networks: Multihop Performance,” IEEE Trans. Mobile Computing, vol. 2, no. 4, pp. 337-348, Oct.-Dec. 2003.

